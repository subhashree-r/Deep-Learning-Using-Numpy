{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)2)3) - The below code does policy iteration and value iteration. The values for states after value iteration is same for value iteration (about 0.99) and policy iteration (about 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The values at every policy update', 0.0)\n",
      "('The values at every policy update', 0.0)\n",
      "('1. Values of states at convergence for value iteration', {(0, 1): 0.0, (1, 2): 0.0, (3, 2): 99.99897095698557, (0, 0): 0.0, (3, 0): 0.0, (3, 1): -99.99897095698557, (2, 1): 0.0, (1, 1): 0.0, (2, 0): 0.0, (2, 2): 0.0, (0, 2): 0.0})\n",
      "('2.qsa pair for every state', array([[  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [-18.99982403,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ],\n",
      "       [  0.        ,   0.        ,   0.        ,   0.        ]]))\n",
      "('3.The optimal values using policy iteration', {(0, 1): 0.0, (1, 2): 0.0, (3, 2): 98.5219117058566, (0, 0): 0.0, (3, 0): 0.0, (3, 1): -98.5219117058566, (2, 1): 0.0, (0, 2): 0.0, (2, 0): 0.0, (2, 2): 0.0, (1, 1): 0.0})\n"
     ]
    }
   ],
   "source": [
    "#Reference -- Introduction to AI by Russel exercise (AIMA)\n",
    "\n",
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "orientations = [(1,0), (0, 1), (-1, 0), (0, -1)]#The different orientations are right, up,left, down\n",
    "class MDP:\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions={}, states=None, gamma=.9):\n",
    "        if states:\n",
    "            self.states = states\n",
    "        else:\n",
    "            self.states = set()\n",
    "        self.init = init\n",
    "        self.actlist = actlist\n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions\n",
    "        self.gamma = gamma\n",
    "        self.reward = {}\n",
    "\n",
    "    def R(self, state):\n",
    "        return self.reward[state]\n",
    "    def T(self, state, action):\n",
    "        if(self.transitions == {}):\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "    def actions(self, state):\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "class GridMDP(MDP):\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
    "        MDP.__init__(self, init, actlist=orientations,\n",
    "                     terminals=terminals, gamma=gamma)\n",
    "        self.grid = grid\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x, y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add((x, y))\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state,orientations[orientations.index(action)-1])),\n",
    "                    (0.1, self.go(state, orientations[(orientations.index(action)+1) % len(orientations)]))]\n",
    "    def go(self, state, direction):\n",
    "        state1 = tuple(operator.add(state,direction))\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "mdp = GridMDP([[0, 0, 0, +10],\n",
    "                     [0, 0,  0, -10],\n",
    "                     [0, None, 0, 0]],\n",
    "                    terminals=[(0, 3), (1, 3)])\n",
    "def argmin(seq, fn):\n",
    "    best = seq[0]; best_score = fn(best)\n",
    "    for x in seq:\n",
    "        x_score = fn(x)\n",
    "        if x_score < best_score:\n",
    "            best, best_score = x, x_score\n",
    "    return best\n",
    "def argmax(seq, fn):\n",
    "    return argmin(seq, lambda x: -fn(x))\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    q = np.zeros((12,4))\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "#         qsa=\n",
    "        for s in mdp.states:\n",
    "#             qsa.append([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "#                                         for a in mdp.actions(s)])\n",
    "#             new_q = qsa + alpha * (rsa + gamma * max(q[next_state, :]) - qsa)\n",
    "            # Modified code to get q values for every state and action\n",
    "            for a in mdp.actions(s):\n",
    "                qsa = q[s,a]\n",
    "                new_q = qsa+0.1*(R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])-qsa)\n",
    "#             print(new_q)\n",
    "                q[s, a] = new_q\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "#             print(U1[s])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "#         qsa=np.array(q)\n",
    "        \n",
    "        \n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            \n",
    "                return U,q\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    U = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        print(\"The values at every policy update\",sum(U.values()))\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return pi,U\n",
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum([p * U[s1] for (p, s1) in T(s, pi[s])])\n",
    "    return U\n",
    "\n",
    "\n",
    "U,qsa= value_iteration(mdp)\n",
    "P,Up =policy_iteration(mdp)\n",
    "print(\"1. Values of states at convergence for value iteration\",U)\n",
    "print(\"2.qsa pair for every state\", qsa)\n",
    "print(\"3.The optimal values using policy iteration\",Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) The below code is when discount factor is 0.5 . As it can be seen the values for the state is reduced. This could be because a lesser discount factor indicates that the future rewards must be given less importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The values at every policy update', 0.0)\n",
      "('The values at every policy update', 0.0)\n",
      "('1. Values of states at convergence for value iteration for discount factor = 0.5', {(0, 1): 0.0, (1, 2): 0.0, (3, 2): 19.998779296875, (0, 0): 0.0, (3, 0): 0.0, (3, 1): -19.998779296875, (2, 1): 0.0, (1, 1): 0.0, (2, 0): 0.0, (2, 2): 0.0, (0, 2): 0.0})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "orientations = [(1,0), (0, 1), (-1, 0), (0, -1)]#The different orientations are right, up,left, down\n",
    "class MDP:\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions={}, states=None, gamma=.5):\n",
    "        if states:\n",
    "            self.states = states\n",
    "        else:\n",
    "            self.states = set()\n",
    "        self.init = init\n",
    "        self.actlist = actlist\n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions\n",
    "        self.gamma = gamma\n",
    "        self.reward = {}\n",
    "\n",
    "    def R(self, state):\n",
    "        return self.reward[state]\n",
    "    def T(self, state, action):\n",
    "        if(self.transitions == {}):\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "    def actions(self, state):\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "class GridMDP(MDP):\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.5):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
    "        MDP.__init__(self, init, actlist=orientations,\n",
    "                     terminals=terminals, gamma=gamma)\n",
    "        self.grid = grid\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x, y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add((x, y))\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return [(1, self.go(state, action)),\n",
    "                    (1, self.go(state,orientations[orientations.index(action)-1])),\n",
    "                    (1, self.go(state, orientations[(orientations.index(action)+1) % len(orientations)]))]\n",
    "    def go(self, state, direction):\n",
    "        state1 = tuple(operator.add(state,direction))\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "mdp = GridMDP([[0, 0, 0, +10],\n",
    "                     [0, 0,  0, -10],\n",
    "                     [0, None, 0, 0]],\n",
    "                    terminals=[(0, 3), (1, 3)])\n",
    "def argmin(seq, fn):\n",
    "    best = seq[0]; best_score = fn(best)\n",
    "    for x in seq:\n",
    "        x_score = fn(x)\n",
    "        if x_score < best_score:\n",
    "            best, best_score = x, x_score\n",
    "    return best\n",
    "def argmax(seq, fn):\n",
    "    return argmin(seq, lambda x: -fn(x))\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        qsa=[]\n",
    "        for s in mdp.states:\n",
    "            qsa.append([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "           \n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        qsa=np.array(qsa)\n",
    "        \n",
    "        \n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            \n",
    "                return U,qsa\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    U = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        print(\"The values at every policy update\",sum(U.values()))\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return pi,U\n",
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum([p * U[s1] for (p, s1) in T(s, pi[s])])\n",
    "    return U\n",
    "\n",
    "\n",
    "U,qsa= value_iteration(mdp)\n",
    "P,Up =policy_iteration(mdp)\n",
    "print(\"1. Values of states at convergence for value iteration for discount factor = 0.5\",U)\n",
    "# print(\"2.qsa pair for every state\", qsa)\n",
    "# print(\"3.The optimal values using policy iteration\",Up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5),6) - The below code is when the reward is modified by changing the reward for every state to -1 and the transition is no longer deterministic. As it can be seen, the values except goal states have become negative which inturn encourages the agent to reach goal in lesser number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The values at every policy update', -79.05810108684881)\n",
      "('The values at every policy update', -88.6697205352709)\n",
      "('1. Values of states at convergence for value iteration', {(0, 1): -9.999897095698548, (1, 2): -9.999897095698548, (3, 2): 99.99897095698557, (0, 0): -9.999897095698548, (3, 0): -9.999897095698548, (3, 1): -99.99897095698557, (2, 1): -9.999897095698548, (1, 1): -9.999897095698548, (2, 0): -9.999897095698548, (2, 2): -9.999897095698548, (0, 2): -9.999897095698548})\n",
      "('2.qsa pair for every state', array([[ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ],\n",
      "       [ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ],\n",
      "       [ 99.99897096,  99.99897096,  99.99897096,  99.99897096],\n",
      "       [ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ],\n",
      "       [ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ],\n",
      "       [-99.99897096, -99.99897096, -99.99897096, -99.99897096],\n",
      "       [ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ],\n",
      "       [ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ],\n",
      "       [ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ],\n",
      "       [ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ],\n",
      "       [ -9.9998971 ,  -9.9998971 ,  -9.9998971 ,  -9.9998971 ]]))\n",
      "('3.The optimal values using policy iteration', {(0, 1): -9.852191170585657, (1, 2): -9.852191170585657, (3, 2): 98.5219117058566, (0, 0): -9.852191170585657, (3, 0): -9.852191170585657, (3, 1): -98.5219117058566, (2, 1): -9.852191170585657, (0, 2): -9.852191170585657, (2, 0): -9.852191170585657, (2, 2): -9.852191170585657, (1, 1): -9.852191170585657})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import operator\n",
    "import numpy as np\n",
    "orientations = [(1,0), (0, 1), (-1, 0), (0, -1)]#The different orientations are right, up,left, down\n",
    "class MDP:\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions={}, states=None, gamma=.9):\n",
    "        if states:\n",
    "            self.states = states\n",
    "        else:\n",
    "            self.states = set()\n",
    "        self.init = init\n",
    "        self.actlist = actlist\n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions\n",
    "        self.gamma = gamma\n",
    "        self.reward = {}\n",
    "\n",
    "    def R(self, state):\n",
    "        return self.reward[state]\n",
    "    def T(self, state, action):\n",
    "        if(self.transitions == {}):\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "    def actions(self, state):\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "class GridMDP(MDP):\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
    "        MDP.__init__(self, init, actlist=orientations,\n",
    "                     terminals=terminals, gamma=gamma)\n",
    "        self.grid = grid\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x, y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add((x, y))\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return [(0.8, self.go(state, action)),\n",
    "                    (0.1, self.go(state,orientations[orientations.index(action)-1])),\n",
    "                    (0.1, self.go(state, orientations[(orientations.index(action)+1) % len(orientations)]))]\n",
    "    def go(self, state, direction):\n",
    "        state1 = tuple(operator.add(state,direction))\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "mdp = GridMDP([[-1, -1, -1, +10],\n",
    "                     [-1, -1,  -1, -10],\n",
    "                     [-1, None, -1, -1]],\n",
    "                    terminals=[(0, 3), (1, 3)])\n",
    "def argmin(seq, fn):\n",
    "    best = seq[0]; best_score = fn(best)\n",
    "    for x in seq:\n",
    "        x_score = fn(x)\n",
    "        if x_score < best_score:\n",
    "            best, best_score = x, x_score\n",
    "    return best\n",
    "def argmax(seq, fn):\n",
    "    return argmin(seq, lambda x: -fn(x))\n",
    "def value_iteration(mdp, epsilon=0.001):\n",
    "\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        qsa=[]\n",
    "        for s in mdp.states:\n",
    "            qsa.append([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "           \n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        qsa=np.array(qsa)\n",
    "        \n",
    "        \n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            \n",
    "                return U,qsa\n",
    "\n",
    "def expected_utility(a, s, U, mdp):\n",
    "    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])\n",
    "\n",
    "def policy_iteration(mdp):\n",
    "    U = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        print(\"The values at every policy update\",sum(U.values()))\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return pi,U\n",
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum([p * U[s1] for (p, s1) in T(s, pi[s])])\n",
    "    return U\n",
    "\n",
    "\n",
    "U,qsa= value_iteration(mdp)\n",
    "P,Up =policy_iteration(mdp)\n",
    "print(\"1. Values of states at convergence for value iteration\",U)\n",
    "print(\"2.qsa pair for every state\", qsa)\n",
    "print(\"3.The optimal values using policy iteration\",Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
