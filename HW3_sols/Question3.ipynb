{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Q values after iterating through the table once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.2\n",
      "0.3\n",
      "1.29\n",
      "0.19\n",
      "0.333697478992\n",
      "1.16441915396\n",
      "0.420227614344\n",
      "0.489635183668\n",
      "0.857087389567\n",
      "0.463646983128\n",
      "0.523585290177\n",
      "0.571473780274\n",
      "0.657349166891\n",
      "0.09\n",
      "1.07767196251\n",
      "0.732144997939\n",
      "0.547211092915\n",
      "0.628800015564\n",
      "0.147643840557\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "qsa = np.zeros((2,2))\n",
    "\n",
    "q = np.zeros((2,2))\n",
    "def q_learn(state, next_state, action, reward, alpha, gamma):\n",
    "\n",
    "    rsa = reward\n",
    "    qsa = q[state, action]\n",
    "    new_q = qsa + alpha * (rsa + gamma * max(q[next_state, :]) - qsa)\n",
    "    print(new_q)\n",
    "    q[state, action] = new_q\n",
    "\n",
    "    # renormalize row to be between 0 and 1\n",
    "    rn = q[state][q[state] > 0] / np.sum(q[state][q[state] > 0])\n",
    "    q[state][q[state] > 0] = rn\n",
    "    return rn\n",
    "\n",
    "\n",
    "# dat = open(\"q-learning.dat\").readlines()\n",
    "dat = [line.split() for line in open(\"q-learning.dat\")]\n",
    "datNew =[]\n",
    "for d in dat:\n",
    "    d = map(int,d)\n",
    "    datNew.append(d)\n",
    "# datNew\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "qIter = 0\n",
    "for sample in datNew:\n",
    "    state = sample[0]-1\n",
    "    action = sample[1]-1\n",
    "    next_state= sample[2]-1\n",
    "    reward = sample[3]\n",
    "    q_learn(state, next_state, action, reward, alpha, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)Iterating through Q table, normalizing the reward every step. The convergence is determined when the reward is high. The corresponding Q values are reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q value at convergence 1.29\n",
      "Q value at convergence 1.16441915396\n",
      "Q value at convergence 1.07767196251\n",
      "Q value at convergence 1.02486139801\n",
      "Q value at convergence 1.04141724634\n",
      "Q value at convergence 1.01070812437\n",
      "Q value at convergence 1.00149870486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "qsa = np.zeros((2,2))\n",
    "\n",
    "q = np.zeros((2,2))\n",
    "def q_learn(state, next_state, action, reward, alpha, gamma):\n",
    "\n",
    "    rsa = reward\n",
    "    qsa = q[state, action]\n",
    "    new_q = qsa + alpha * (rsa + gamma * max(q[next_state, :]) - qsa)\n",
    "#     print(new_q)\n",
    "    q[state, action] = new_q\n",
    "\n",
    "    # renormalize row to be between 0 and 1\n",
    "    rn = q[state][q[state] > 0] / np.sum(q[state][q[state] > 0])\n",
    "    q[state][q[state] > 0] = rn\n",
    "    return q, new_q\n",
    "\n",
    "\n",
    "# dat = open(\"q-learning.dat\").readlines()\n",
    "dat = [line.split() for line in open(\"q-learning.dat\")]\n",
    "datNew =[]\n",
    "for d in dat:\n",
    "    d = map(int,d)\n",
    "    datNew.append(d)\n",
    "# datNew\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "qIter = 0\n",
    "for _ in range(100):\n",
    "    for sample in datNew:\n",
    "        state = sample[0]-1\n",
    "        action = sample[1]-1\n",
    "        next_state= sample[2]-1\n",
    "        reward = sample[3]\n",
    "        q,qv = q_learn(state, next_state, action, reward, alpha, gamma)\n",
    "#         print(q,qv)\n",
    "        if qv > 1:\n",
    "            print(\"Q value at convergence {0}\".format(qv))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) The alpha is changed by 1/w+1 where w is the iteration. Hence the learning rate is slowly decreased. As can be seen, The running average has a higher rate of convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.725\n",
      "0.38\n",
      "0.5461352657\n",
      "1.21617291661\n",
      "0.56519673066\n",
      "0.603805697386\n",
      "0.757801014102\n",
      "0.51914109125\n",
      "0.55541085431\n",
      "0.58552437458\n",
      "0.642434808433\n",
      "0.06\n",
      "1.04238351903\n",
      "0.597345561086\n",
      "0.548402132377\n",
      "0.590811712641\n",
      "0.0916123816339\n"
     ]
    }
   ],
   "source": [
    "# When alpha is changing\n",
    "import numpy as np\n",
    "from __future__ import division\n",
    "qsa = np.zeros((2,2))\n",
    "\n",
    "q = np.zeros((2,2))\n",
    "def q_learn(state, next_state, action, reward, alpha, gamma):\n",
    "    global qIter\n",
    "    qIter+=1\n",
    "    alpha = 1/qIter\n",
    "#     print(qIter,alpha)\n",
    "    rsa = reward\n",
    "    qsa = q[state, action]\n",
    "    new_q = qsa + alpha * (rsa + gamma * max(q[next_state, :]) - qsa)\n",
    "    print(new_q)\n",
    "    q[state, action] = new_q\n",
    "    rn = q[state][q[state] > 0] / np.sum(q[state][q[state] > 0])\n",
    "    q[state][q[state] > 0] = rn\n",
    "    return rn\n",
    "\n",
    "dat = [line.split() for line in open(\"q-learning.dat\")]\n",
    "datNew =[]\n",
    "for d in dat:\n",
    "    d = map(int,d)\n",
    "    datNew.append(d)\n",
    "# datNew\n",
    "alpha=0.1\n",
    "gamma=0.9\n",
    "qIter = 0\n",
    "for sample in datNew:\n",
    "    state = sample[0]-1\n",
    "    action = sample[1]-1\n",
    "    next_state= sample[2]-1\n",
    "    reward = sample[3]\n",
    "    q_learn(state, next_state, action, reward, alpha, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
