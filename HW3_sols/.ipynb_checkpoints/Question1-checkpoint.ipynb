{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The codes below does sequence predictedWordiction **\n",
    "1. When Number of LSTM =1,2 ,3\n",
    "2.The preiction of 50 words when the input sequence length =2,5,10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code has 1 layer LSTM, and input sequence = 5\n",
    "Observations:\n",
    "The training predictedWordiction accuracy increases with number of iterations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2041,)\n",
      "Reading the text\n",
      "Iter= 5000Training Accuracy= 3.06%\n",
      "Iter= 10000Training Accuracy= 6.62%\n",
      "Iter= 15000Training Accuracy= 16.38%\n",
      "Iter= 20000Training Accuracy= 28.14%\n",
      "Iter= 25000Training Accuracy= 24.22%\n",
      "Iter= 30000Training Accuracy= 42.88%\n",
      "Iter= 35000Training Accuracy= 49.46%\n",
      "Iter= 40000Training Accuracy= 45.54%\n",
      "Iter= 45000Training Accuracy= 49.24%\n",
      "Iter= 50000Training Accuracy= 53.24%\n",
      "Training Over for 1-layer LSTM\n",
      "Output Random sentence following a probe by the us securities and downfall the amount shares continuing shares . the to full has but to it also reported basis shares to growth % $amount was 123 performance to $amount at has half to a creditors owns to year earlier action to sale , had has\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "training_file = 'text-Copy1.txt'\n",
    "input_file =  'text-Copy1.txt'\n",
    "with codecs.open(input_file, \"r\", encoding=None) as f:\n",
    "    data = f.read()\n",
    "\n",
    "    \n",
    "############################ Reading the text file and tokenizing every word ##########################\n",
    "x_text = data.split()\n",
    "totalWords = len(x_text)\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    print(content.shape)\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Reading the text\")\n",
    "\n",
    "def vocabularyBuild(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    wordToInt = dict()\n",
    "    for word, _ in count:\n",
    "        wordToInt[word] = len(wordToInt)\n",
    "    intToWord = dict(zip(wordToInt.values(), wordToInt.input_words()))\n",
    "    return wordToInt, intToWord\n",
    "\n",
    "wordToInt, intToWord = vocabularyBuild(training_data)\n",
    "vocab_size = len(wordToInt)\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "frequency = 5000\n",
    "n_input = 5\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# The final hidden state of RNN\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocab_size]))}\n",
    "\n",
    "########### defining the RNN Model ######################################################3\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    x = tf.split(x,n_input,1)\n",
    "    #Define RNN for a single LSTM cell\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden)])\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "predictedWord = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictedWord, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_predictedWord = tf.equal(tf.argmax(predictedWord,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictedWord, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    while step < training_iters:\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "        #At every iteration a random index is chosen and n_input words are chosen for input\n",
    "        inputSequence = [ [wordToInt[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        inputSequence = np.reshape(np.array(inputSequence), [-1, n_input, 1])\n",
    "        #Encode the input sequence to integers\n",
    "        encodedOutput = np.zeros([vocab_size], dtype=float)\n",
    "        encodedOutput[wordToInt[str(training_data[offset+n_input])]] = 1.0\n",
    "        encodedOutput = np.reshape(encodedOutput,[1,-1])\n",
    "        opt, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, predictedWord], \\\n",
    "                                                feed_dict={x: inputSequence, y: encodedOutput})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % frequency == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \"Training Accuracy= \" + \"{:.2f}%\".format(100*acc_total/frequency))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            inSeq = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            outSeq = training_data[offset + n_input]\n",
    "            predictedSeq = intToWord[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Training Over for 1-layer LSTM\")\n",
    "    start_index = random.randint(1,1900)\n",
    "    sentence = 'following a probe by the'\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    inputSequence = [wordToInt[words[i]] for i in range(len(words))]\n",
    "    for i in range(50):\n",
    "#         print(i)\n",
    "        input_words = np.reshape(np.array(inputSequence), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(predictedWord, feed_dict={x: input_words})\n",
    "        predictedWordIndex = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        sentence = \"%s %s\" % (sentence,intToWord[predictedWordIndex])\n",
    "        inputSequence = inputSequence[1:]\n",
    "        inputSequence.append(predictedWordIndex)\n",
    "        nw = sentence.split(' ')\n",
    "        if len(nw)>50:\n",
    "            print(\"Output Random sentence\",sentence)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The below code has 2 layer LSTM with, input sequence =5\n",
    "As expected, the accuracy during training is higher compared to 1 layer LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2041,)\n",
      "Reading the text\n",
      "Iter= 5000Training Accuracy= 4.20%\n",
      "Iter= 10000Training Accuracy= 4.52%\n",
      "Iter= 15000Training Accuracy= 14.88%\n",
      "Iter= 20000Training Accuracy= 16.18%\n",
      "Iter= 25000Training Accuracy= 15.84%\n",
      "Iter= 30000Training Accuracy= 23.98%\n",
      "Iter= 35000Training Accuracy= 33.36%\n",
      "Iter= 40000Training Accuracy= 42.52%\n",
      "Iter= 45000Training Accuracy= 59.52%\n",
      "Iter= 50000Training Accuracy= 61.42%\n",
      "Training Over for 1-layer LSTM\n",
      "Output Random sentence following a probe by the us securities deficit is shows up a a $amount drop a an pernod a said friday . rod of a wall deficits google in a is court in an pernod a said friday . rod of a wall deficits google in a is court in an\n"
     ]
    }
   ],
   "source": [
    "#2 layer\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "start_time = time.time()\n",
    "training_file = 'text-Copy1.txt'\n",
    "input_file =  'text-Copy1.txt'\n",
    "with codecs.open(input_file, \"r\", encoding=None) as f:\n",
    "    data = f.read()\n",
    "\n",
    "x_text = data.split()\n",
    "totalWords = len(x_text)\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    print(content.shape)\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Reading the text\")\n",
    "\n",
    "def vocabularyBuild(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    wordToInt = dict()\n",
    "    for word, _ in count:\n",
    "        wordToInt[word] = len(wordToInt)\n",
    "    intToWord = dict(zip(wordToInt.values(), wordToInt.input_words()))\n",
    "    return wordToInt, intToWord\n",
    "\n",
    "wordToInt, intToWord = vocabularyBuild(training_data)\n",
    "vocab_size = len(wordToInt)\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "frequency = 5000\n",
    "n_input = 5\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# The final hidden state of RNN\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocab_size]))}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    x = tf.split(x,n_input,1)\n",
    "    #Define RNN for a single LSTM cell\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "predictedWord = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictedWord, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_predictedWord = tf.equal(tf.argmax(predictedWord,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictedWord, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    while step < training_iters:\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "        inputSequence = [ [wordToInt[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        inputSequence = np.reshape(np.array(inputSequence), [-1, n_input, 1])\n",
    "        encodedOutput = np.zeros([vocab_size], dtype=float)\n",
    "        encodedOutput[wordToInt[str(training_data[offset+n_input])]] = 1.0\n",
    "        encodedOutput = np.reshape(encodedOutput,[1,-1])\n",
    "        opt, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, predictedWord], feed_dict={x: inputSequence, y: encodedOutput})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % frequency == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \"Training Accuracy= \" + \"{:.2f}%\".format(100*acc_total/frequency))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            inSeq = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            outSeq = training_data[offset + n_input]\n",
    "            predictedSeq = intToWord[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Training Over for 1-layer LSTM\")\n",
    "    start_index = random.randint(1,1900)\n",
    "    sentence = 'following a probe by the'\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    inputSequence = [wordToInt[words[i]] for i in range(len(words))]\n",
    "    for i in range(50):\n",
    "#         print(i)\n",
    "        input_words = np.reshape(np.array(inputSequence), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(predictedWord, feed_dict={x: input_words})\n",
    "        predictedWordIndex = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        sentence = \"%s %s\" % (sentence,intToWord[predictedWordIndex])\n",
    "        inputSequence = inputSequence[1:]\n",
    "        inputSequence.append(predictedWordIndex)\n",
    "        nw = sentence.split(' ')\n",
    "        if len(nw)>50:\n",
    "            print(\"Output Random sentence\",sentence)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The below code is 3 layer LSTM for 2 input sequence.\n",
    " As it can be seen, the training acuracy increases slowly in comparison to above model. The reason could be due to a smaller input sequence due to which the time dependency learnt is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2041,)\n",
      "Reading the text\n",
      "Iter= 5000Training Accuracy= 4.02%\n",
      "Iter= 10000Training Accuracy= 5.20%\n",
      "Iter= 15000Training Accuracy= 5.90%\n",
      "Iter= 20000Training Accuracy= 7.56%\n",
      "Iter= 25000Training Accuracy= 7.04%\n",
      "Iter= 30000Training Accuracy= 7.52%\n",
      "Iter= 35000Training Accuracy= 8.54%\n",
      "Iter= 40000Training Accuracy= 13.10%\n",
      "Iter= 45000Training Accuracy= 11.42%\n",
      "Iter= 50000Training Accuracy= 14.54%\n",
      "Training Over for 3-layer LSTM\n",
      "Output sentence for input of 2 words dollar higher to its main demand , a major in pre owed , a major in pre owed , a major in pre owed , a major in pre owed , a major in pre owed , a major in pre owed , a major in pre owed , a major\n"
     ]
    }
   ],
   "source": [
    "#3 layer and input length is 2\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "start_time = time.time()\n",
    "training_file = 'text-Copy1.txt'\n",
    "input_file =  'text-Copy1.txt'\n",
    "with codecs.open(input_file, \"r\", encoding=None) as f:\n",
    "    data = f.read()\n",
    "\n",
    "x_text = data.split()\n",
    "totalWords = len(x_text)\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    print(content.shape)\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Reading the text\")\n",
    "\n",
    "def vocabularyBuild(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    wordToInt = dict()\n",
    "    for word, _ in count:\n",
    "        wordToInt[word] = len(wordToInt)\n",
    "    intToWord = dict(zip(wordToInt.values(), wordToInt.input_words()))\n",
    "    return wordToInt, intToWord\n",
    "\n",
    "wordToInt, intToWord = vocabularyBuild(training_data)\n",
    "vocab_size = len(wordToInt)\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "frequency = 5000\n",
    "n_input = 2\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# The final hidden state of RNN\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocab_size]))}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    x = tf.split(x,n_input,1)\n",
    "    #Define RNN for a single LSTM cell\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "predictedWord = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictedWord, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_predictedWord = tf.equal(tf.argmax(predictedWord,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictedWord, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    while step < training_iters:\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "        inputSequence = [ [wordToInt[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        inputSequence = np.reshape(np.array(inputSequence), [-1, n_input, 1])\n",
    "        encodedOutput = np.zeros([vocab_size], dtype=float)\n",
    "        encodedOutput[wordToInt[str(training_data[offset+n_input])]] = 1.0\n",
    "        encodedOutput = np.reshape(encodedOutput,[1,-1])\n",
    "        opt, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, predictedWord], feed_dict={x: inputSequence, y: encodedOutput})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % frequency == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \"Training Accuracy= \" + \"{:.2f}%\".format(100*acc_total/frequency))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            inSeq = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            outSeq = training_data[offset + n_input]\n",
    "            predictedSeq = intToWord[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Training Over for 3-layer LSTM\")\n",
    "    start_index = random.randint(1,1900)\n",
    "    sentence = 'dollar higher'\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    inputSequence = [wordToInt[words[i]] for i in range(len(words))]\n",
    "    for i in range(50):\n",
    "#         print(i)\n",
    "        input_words = np.reshape(np.array(inputSequence), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(predictedWord, feed_dict={x: input_words})\n",
    "        predictedWordIndex = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        sentence = \"%s %s\" % (sentence,intToWord[predictedWordIndex])\n",
    "        inputSequence = inputSequence[1:]\n",
    "        inputSequence.append(predictedWordIndex)\n",
    "        nw = sentence.split(' ')\n",
    "        if len(nw)>50:\n",
    "            print(\"Output sentence for input of 2 words\",sentence)\n",
    "            break\n",
    "    \n",
    "#     words =\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) The below code does predictedWordiction using 3 layer of LSTM for an input sequence of length 10. The training accuracy is very low for 50000 iterations. Also, the generated sentence gives empty string. This could be because the network was unable to learn a long sequence and probably requires more number of iterations. Also, vanishing gradient could have been a problem due to a longer sequence into the past. This problem could be overcome by gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2041,)\n",
      "Reading the text\n",
      "Iter= 5000Training Accuracy= 3.58%\n",
      "Iter= 10000Training Accuracy= 3.66%\n",
      "Iter= 15000Training Accuracy= 4.10%\n",
      "Iter= 20000Training Accuracy= 3.90%\n",
      "Iter= 25000Training Accuracy= 3.08%\n",
      "Iter= 30000Training Accuracy= 3.14%\n",
      "Iter= 35000Training Accuracy= 3.00%\n",
      "Iter= 40000Training Accuracy= 3.08%\n",
      "Iter= 45000Training Accuracy= 2.64%\n",
      "Iter= 50000Training Accuracy= 3.12%\n",
      "Training Over for 3-layer LSTM\n",
      "Output sentence for input of 10 words yukos claims its downfall was punishment for the political ambitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "#3 layer and input sequence length 10\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import codecs\n",
    "\n",
    "start_time = time.time()\n",
    "training_file = 'text-Copy1.txt'\n",
    "input_file =  'text-Copy1.txt'\n",
    "with codecs.open(input_file, \"r\", encoding=None) as f:\n",
    "    data = f.read()\n",
    "\n",
    "x_text = data.split()\n",
    "totalWords = len(x_text)\n",
    "def read_data(fname):\n",
    "    with open(fname) as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content]\n",
    "    content = [content[i].split() for i in range(len(content))]\n",
    "    content = np.array(content)\n",
    "    content = np.reshape(content, [-1, ])\n",
    "    print(content.shape)\n",
    "    return content\n",
    "\n",
    "training_data = read_data(training_file)\n",
    "print(\"Reading the text\")\n",
    "\n",
    "def vocabularyBuild(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    wordToInt = dict()\n",
    "    for word, _ in count:\n",
    "        wordToInt[word] = len(wordToInt)\n",
    "    intToWord = dict(zip(wordToInt.values(), wordToInt.input_words()))\n",
    "    return wordToInt, intToWord\n",
    "\n",
    "wordToInt, intToWord = vocabularyBuild(training_data)\n",
    "vocab_size = len(wordToInt)\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 50000\n",
    "frequency = 5000\n",
    "n_input = 10\n",
    "\n",
    "# number of units in RNN cell\n",
    "n_hidden = 512\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input, 1])\n",
    "y = tf.placeholder(\"float\", [None, vocab_size])\n",
    "\n",
    "# The final hidden state of RNN\n",
    "weights = {'out': tf.Variable(tf.random_normal([n_hidden, vocab_size]))}\n",
    "biases = {'out': tf.Variable(tf.random_normal([vocab_size]))}\n",
    "\n",
    "def RNN(x, weights, biases):\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    x = tf.split(x,n_input,1)\n",
    "    #Define RNN for a single LSTM cell\n",
    "    rnn_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden),rnn.BasicLSTMCell(n_hidden)])\n",
    "    outputs, states = rnn.static_rnn(rnn_cell, x, dtype=tf.float32)\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "predictedWord = RNN(x, weights, biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictedWord, labels=y))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_predictedWord = tf.equal(tf.argmax(predictedWord,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictedWord, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    step = 0\n",
    "    offset = random.randint(0,n_input+1)\n",
    "    end_offset = n_input + 1\n",
    "    acc_total = 0\n",
    "    loss_total = 0\n",
    "    while step < training_iters:\n",
    "        if offset > (len(training_data)-end_offset):\n",
    "            offset = random.randint(0, n_input+1)\n",
    "        inputSequence = [ [wordToInt[ str(training_data[i])]] for i in range(offset, offset+n_input) ]\n",
    "        inputSequence = np.reshape(np.array(inputSequence), [-1, n_input, 1])\n",
    "        encodedOutput = np.zeros([vocab_size], dtype=float)\n",
    "        encodedOutput[wordToInt[str(training_data[offset+n_input])]] = 1.0\n",
    "        encodedOutput = np.reshape(encodedOutput,[1,-1])\n",
    "        opt, acc, loss, onehot_pred = session.run([optimizer, accuracy, cost, predictedWord], feed_dict={x: inputSequence, y: encodedOutput})\n",
    "        loss_total += loss\n",
    "        acc_total += acc\n",
    "        if (step+1) % frequency == 0:\n",
    "            print(\"Iter= \" + str(step+1) + \"Training Accuracy= \" + \"{:.2f}%\".format(100*acc_total/frequency))\n",
    "            acc_total = 0\n",
    "            loss_total = 0\n",
    "            inSeq = [training_data[i] for i in range(offset, offset + n_input)]\n",
    "            outSeq = training_data[offset + n_input]\n",
    "            predictedSeq = intToWord[int(tf.argmax(onehot_pred, 1).eval())]\n",
    "        step += 1\n",
    "        offset += (n_input+1)\n",
    "    print(\"Training Over for 3-layer LSTM\")\n",
    "    start_index = random.randint(1,1900)\n",
    "    sentence = 'yukos claims its downfall was punishment for the political ambitions'\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    inputSequence = [wordToInt[words[i]] for i in range(len(words))]\n",
    "    for i in range(50):\n",
    "#         print(i)\n",
    "        input_words = np.reshape(np.array(inputSequence), [-1, n_input, 1])\n",
    "        onehot_pred = session.run(predictedWord, feed_dict={x: input_words})\n",
    "        predictedWordIndex = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        sentence = \"%s %s\" % (sentence,intToWord[predictedWordIndex])\n",
    "        inputSequence = inputSequence[1:]\n",
    "        inputSequence.append(predictedWordIndex)\n",
    "        nw = sentence.split(' ')\n",
    "        if len(nw)>50:\n",
    "            print(\"Output sentence for input of 10 words\",sentence)\n",
    "            break\n",
    "    \n",
    "#     words =\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (python2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
